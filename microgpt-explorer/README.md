# MicroGPT Visual Explorer

An interactive educational tool that explains how GPT models work, using simple language and clean visualizations. Built to help anyone understand the fundamentals of language modelsâ€”no machine learning background needed.

## About

This site visualizes a tiny GPT model trained on baby names. It breaks down complex concepts into five interactive sections:

1. **Tokenizer** - How text becomes numbers
2. **Embeddings** - How numbers become representations
3. **Forward Pass** - How the model processes information
4. **Training** - How the model learns
5. **Inference** - How the model generates new names

## Features

- ğŸ¨ **Clean, minimal design** - Black and white aesthetic focusing on content
- ğŸ”„ **Interactive visualizations** - Real-time updates as you change inputs
- ğŸ“± **Fully responsive** - Works on desktop, tablet, and mobile
- ğŸŒ“ **Dark mode support** - Automatic theme switching
- â™¿ **Accessible** - Keyboard navigation and semantic HTML
- ğŸ“š **Educational** - Plain language explanations throughout

## Tech Stack

- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Components**: Shadcn UI
- **Charts**: Recharts
- **Animations**: Framer Motion

## Getting Started

### Prerequisites

- Node.js 18+
- npm or yarn

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd microgpt-explorer

# Install dependencies
npm install

# Run the development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

### Build for Production

```bash
npm run build
npm start
```

## Project Structure

```
microgpt-explorer/
â”œâ”€â”€ app/                      # Next.js app directory
â”‚   â”œâ”€â”€ page.tsx             # Homepage
â”‚   â”œâ”€â”€ tokenizer/           # Tokenizer section
â”‚   â”œâ”€â”€ embeddings/          # Embeddings section
â”‚   â”œâ”€â”€ forward-pass/        # Forward pass section
â”‚   â”œâ”€â”€ training/            # Training section
â”‚   â””â”€â”€ inference/           # Inference section
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                  # Shadcn UI components
â”‚   â”œâ”€â”€ layout/              # Layout components
â”‚   â””â”€â”€ sections/            # Section-specific components
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ constants.ts         # Model constants and config
â”‚   â”œâ”€â”€ mock-data.ts         # Mock data generators
â”‚   â”œâ”€â”€ tokenizer.ts         # Tokenizer utilities
â”‚   â””â”€â”€ utils.ts             # Helper functions
â””â”€â”€ types/
    â””â”€â”€ index.ts             # TypeScript type definitions
```

## Sections

### 1. Tokenizer (`/tokenizer`)

Learn how the model converts text into numbers:
- Interactive text input
- Token sequence visualization
- Next-token prediction pairs
- BOS (Beginning of Sequence) markers

### 2. Embeddings (`/embeddings`)

See how tokens become meaningful representations:
- Token and position selectors
- Bar chart visualizations
- Visual addition of embeddings
- 16-dimensional vectors explained

### 3. Forward Pass (`/forward-pass`)

Follow information through the model:
- Step-by-step accordion interface
- Architecture flow diagram
- Key concepts explained (Attention, RMSNorm, MLP, Residuals)
- Position-by-position processing

### 4. Training (`/training`)

Watch the model learn:
- Interactive loss curve
- Play/pause animation
- Real-time metrics display
- Training cycle visualization
- 100-step simulated training

### 5. Inference (`/inference`)

Generate new names:
- Temperature control (0.1 - 1.5)
- Adjustable number of outputs
- Token sequence display
- Copy-to-clipboard functionality

## Design Philosophy

This project follows these design principles:

1. **Clarity over complexity** - Simple explanations, minimal jargon
2. **Black and white aesthetic** - No distracting colors, focus on content
3. **Progressive disclosure** - Information revealed step-by-step
4. **Responsive design** - Mobile-first approach
5. **Accessibility** - WCAG 2.1 AA compliance

## Educational Goals

The app aims to teach:

- **What** language models do (predict next tokens)
- **How** they represent information (embeddings)
- **Why** they work (attention, training, patterns)
- **When** to use different settings (temperature)

All without requiring:
- Math background
- Programming experience
- Machine learning knowledge

## Acknowledgements

Based on [microgpt.py](https://github.com/karpathy/microgpt) by Andrej Karpathy - a minimal, educational implementation of GPT in pure Python.

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

---

Built with â¤ï¸ for anyone curious about how language models work
